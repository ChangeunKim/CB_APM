{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from matplotlib import pyplot as plt\n",
    "import pickle\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "\n",
    "HRIZON = ''         # Defines prediction horizon used to search file names. See run.py for naming rules.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot test results (out-of-sample $R^2$)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "whole_periods  = pd.DataFrame()\n",
    "\n",
    "for file in os.listdir('results'):\n",
    "    if file.startswith(HRIZON) and file.endswith('.csv'):\n",
    "        weight_lambda = float(file.split(HRIZON+'_')[1].split('.csv')[0])\n",
    "        path = os.path.join('results', file)\n",
    "        result = pd.read_csv(path)\n",
    "        whole_periods[weight_lambda] = result['Whole periods']\n",
    "        \n",
    "whole_periods.index = result['Unnamed: 0']\n",
    "whole_periods[0.000]['Consensus average'] = np.nan\n",
    "\n",
    "whole_periods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot out-of-sample R^2 of predicting consensus variables and asset returns for different lambda settings\n",
    "\n",
    "plt.figure()\n",
    "whole_periods.iloc[-1].plot(style='o-', color='orange') # Asset Return\n",
    "whole_periods.iloc[-2].plot(style='s-', color='green')  # Consensus Variablers\n",
    "\n",
    "plt.grid(visible=True)\n",
    "plt.xlabel('$\\\\lambda$')\n",
    "plt.ylabel('$R^2$')\n",
    "plt.xticks([0, 0.002, 0.004, 0.006, 0.008, 0.01]) # lambda\n",
    "plt.legend(['Return', 'Consensus'])\n",
    "\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot out-of-sample R^2 of predicting consensus variables and asset returns for different testing periods\n",
    "\n",
    "columns = ['2014-01-01', '2015-01-01', '2016-01-01', '2017-01-01', '2018-01-01', '2019-01-01', '2020-01-01', '2021-01-01', '2022-01-01', '2023-01-01']\n",
    "index_name = ['2013-01-01~2013-12-31','2014-01-01~2014-12-31', '2015-01-01~2015-12-31', '2016-01-01~2016-12-31', '2017-01-01~2017-12-31',\n",
    "            '2018-01-01~2018-12-31', '2019-01-01~2019-12-31', '2020-01-01~2020-12-31', '2021-01-01~2021-12-31', '2022-01-01~2022-12-31']\n",
    "i = 0\n",
    "fig, axs = plt.subplots(nrows=5, ncols=2, sharex=True, sharey=True, figsize=(10, 15))\n",
    "\n",
    "\n",
    "for file in os.listdir('results'):\n",
    "    if file.startswith(HRIZON) and file.endswith('.csv'):\n",
    "        weight_lambda = float(file.split(HRIZON+'_')[1].split('.csv')[0]) # Parse lambda from file name\n",
    "        path = os.path.join('results', file)\n",
    "        result = pd.read_csv(path)\n",
    "\n",
    "        if weight_lambda != 0.0: # Ignore naive feedforward network\n",
    "            index = np.arange(len(columns))\n",
    "            ax = fig.axes[i]        \n",
    "            ax.bar(index-0.4, result.iloc[-1][columns], width=0.4, align='center', label='Return')\n",
    "            ax.bar(index, result.iloc[-2][columns], width=0.4, align='center', label='Consensus')\n",
    "            if i == 0:\n",
    "                ax.legend()\n",
    "            ax.set_xticks(index-0.3)\n",
    "            ax.set_xticklabels(tuple(index_name), rotation=45)\n",
    "            ax.grid(axis=\"y\")\n",
    "            ax.set_title(f\"$\\\\lambda =$ {str(weight_lambda)}\")\n",
    "            i = i+1\n",
    "\n",
    "fig.tight_layout()\n",
    "\n",
    "for ax in fig.get_axes():\n",
    "    ax.label_outer()\n",
    "\n",
    "fig.supylabel(\"$R^2$\", size = 15)\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate portfolio performance and plot cumulative returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, load variables required for portfolio performance metrics\n",
    "\n",
    "# Risk free rates to calculate excess returns\n",
    "\n",
    "welch_goyal = pd.read_csv('data/raw/welch_goyal_raw.csv')\n",
    "welch_goyal['yyyymm'] = pd.to_datetime(welch_goyal['yyyymm'], format = '%Y%m')\n",
    "welch_goyal = welch_goyal.rename(columns={'yyyymm':'date'})\n",
    "risk_free = welch_goyal[['date', 'Rfree']]\n",
    "risk_free"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SNP500 log returns for benchmark\n",
    "\n",
    "snp_500 = welch_goyal[['date', 'Index']]\n",
    "snp_500['return'] = np.log(snp_500['Index']) - np.log(snp_500['Index'].shift(1)) # Log return\n",
    "snp_500.drop('Index', axis=1, inplace=True)\n",
    "snp_500 = snp_500[(snp_500['date']>=pd.to_datetime('2013-01-01')) & (snp_500['date']<pd.to_datetime('2023-01-01'))].set_index('date')\n",
    "snp_500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stock price and market cap for value-weighted portfolios\n",
    "\n",
    "CZD = pd.read_csv('data/raw/open_source_asset_pricing.csv')\n",
    "\n",
    "stock_info = CZD[['date', 'permno', 'Size', 'Price']]\n",
    "stock_info['date'] = pd.to_datetime(stock_info['date'])\n",
    "stock_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "portfolio_summary = dict()\n",
    "portfolio_summary['return'] = pd.DataFrame()\n",
    "portfolio_summary['weight'] = dict()\n",
    "portfolio_summary['returns'] = dict()\n",
    "\n",
    "consensus = True  # Choose whether to utilize predicted consensus variables to douvble sort long-short portfolios\n",
    "\n",
    "color = iter(plt.cm.rainbow(np.linspace(0, 1, 11)))\n",
    "\n",
    "plt.figure(figsize=(17, 10))\n",
    "\n",
    "for file in os.listdir('results'):\n",
    "    if file.startswith('approx') and file.endswith('.pickle'): # Only consider case of 1 month prediction\n",
    "        weight_lambda = float(file.split('approx_')[1].split('.pickle')[0])\n",
    "        \n",
    "        path = os.path.join('results', file)\n",
    "        with open(file=path, mode='rb') as f:\n",
    "            output = pickle.load(f)\n",
    "\n",
    "        actual = output['actual_target']\n",
    "        actual.columns = ['date', 'permno', 'actual']\n",
    "        forecast = output['forecast_target']\n",
    "        forecast.columns = ['date', 'permno', 'forecast']\n",
    "\n",
    "        returns = pd.merge(actual, forecast).dropna()\n",
    "\n",
    "        if weight_lambda != 0 and consensus:\n",
    "            consensuses = output['forecast_concept']\n",
    "            consensuses.columns = ['date', 'permno', 'EPS forecast revision', 'Change in recommendation', 'Change in Forecast and Accrual', 'Long-vs-short EPS forecasts', \n",
    "            'Analyst earnings per share', 'EPS Forecast Dispersion', 'Earnings forecast revisions', 'Analyst Value', 'Analyst Optimism']\n",
    "            cons_var = 'Analyst earnings per share' # Choose which consensus variable to utilize for double sort\n",
    "            returns = pd.merge(returns, consensuses[['date', 'permno', cons_var]], how='left')\n",
    "        returns = pd.merge(returns, stock_info, how='left')\n",
    "\n",
    "        portfolio_returns = list()\n",
    "        individual_returns = pd.DataFrame()\n",
    "        portfolio_weights = pd.DataFrame()\n",
    "\n",
    "        for snapshot in returns.groupby('date'):\n",
    "            date = snapshot[0]\n",
    "            curr_returns = snapshot[1]\n",
    "\n",
    "            individual_returns = pd.concat([individual_returns, curr_returns[['date', 'permno', 'actual']].set_index('date')])\n",
    "\n",
    "            if weight_lambda != 0 and consensus:\n",
    "                # Get return threshold for each positions by percentile\n",
    "                long_cut  = curr_returns.quantile(0.75)['forecast']\n",
    "                short_cut = curr_returns.quantile(0.25)['forecast']\n",
    "                # Get list of firms for each positions\n",
    "                long_posit  = curr_returns[curr_returns['forecast']>=long_cut ]['permno']\n",
    "                short_posit = curr_returns[curr_returns['forecast']<=short_cut]['permno']\n",
    "\n",
    "                # Do the same for consensus variable\n",
    "                long_con  = curr_returns[curr_returns['permno'].isin(long_posit)].quantile(0.75)[cons_var]\n",
    "                short_con = curr_returns[curr_returns['permno'].isin(short_posit)].quantile(0.25)[cons_var]\n",
    "                long_posit  = curr_returns[curr_returns['permno'].isin(long_posit)][curr_returns[cons_var]>=long_con ]['permno']\n",
    "                short_posit = curr_returns[curr_returns['permno'].isin(short_posit)][curr_returns[cons_var]<=short_con]['permno']\n",
    "            else:\n",
    "                # Sort portfolio only once when (consensus == False)\n",
    "                long_cut  = curr_returns.quantile(0.9)['forecast']\n",
    "                short_cut = curr_returns.quantile(0.1)['forecast']\n",
    "                long_posit  = curr_returns[curr_returns['forecast']>=long_cut ]['permno']\n",
    "                short_posit = curr_returns[curr_returns['forecast']<=short_cut]['permno']\n",
    "            \n",
    "            # Get portfolio weights (value-weighted)\n",
    "            long_total  = curr_returns[curr_returns['permno'].isin(long_posit)]['Size'].sum()\n",
    "            short_total = curr_returns[curr_returns['permno'].isin(short_posit)]['Size'].sum()\n",
    "            long_weight  = curr_returns[curr_returns['permno'].isin(long_posit)]['Size']/long_total\n",
    "            short_weight = -curr_returns[curr_returns['permno'].isin(short_posit)]['Size']/short_total\n",
    "\n",
    "            portfolio_weight = pd.concat([long_weight, short_weight])\n",
    "            portfolio_weight.name = 'weight'\n",
    "            portfolio_weight = curr_returns.join(portfolio_weight, how='left')[['date', 'permno', 'weight']].fillna(0).set_index('date')\n",
    "            portfolio_weights = pd.concat([portfolio_weights, portfolio_weight])\n",
    "            \n",
    "            # Get portfolio returns for each positions\n",
    "            long_return = curr_returns[curr_returns['permno'].isin(long_posit)]['actual'].values * long_weight\n",
    "            short_return = curr_returns[curr_returns['permno'].isin(short_posit)]['actual'].values * short_weight\n",
    "\n",
    "            portfolio_return = pd.concat([long_return, short_return])\n",
    "            portfolio_returns.append(portfolio_return.sum())\n",
    "\n",
    "        \n",
    "        # Plot portfolio cumulative log returns for different lambda settings\n",
    "        portfolio = pd.DataFrame()\n",
    "        portfolio.index = returns['date'].unique()\n",
    "        portfolio['return'] = portfolio_returns\n",
    "        portfolio['$\\\\lambda=$'+str(weight_lambda)] = np.exp(portfolio['return']).cumprod()-1\n",
    "        c = next(color)\n",
    "        portfolio['$\\\\lambda=$'+str(weight_lambda)].plot(legend=True, color=c)\n",
    "\n",
    "        portfolio_summary['return'][weight_lambda]  = portfolio_returns\n",
    "        portfolio_summary['weight'][weight_lambda]  = portfolio_weights\n",
    "        portfolio_summary['returns'][weight_lambda] = individual_returns\n",
    "\n",
    "portfolio_summary['return']['S&P 500'] = snp_500['return'].values\n",
    "portfolio_summary['return']['date'] = returns['date'].unique()\n",
    "\n",
    " # Plot portfolio cumulative log return for s&p500 buy-and-hold strategy as a benchmark\n",
    "snp_500['S&P 500'] = np.exp(snp_500['return']).cumprod()-1\n",
    "snp_500['S&P 500'].plot(legend=True, style='--', color='black')\n",
    "\n",
    "plt.grid()\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function for calculating maximum drawdown\n",
    "def MAX_DD(data):\n",
    "    max_dd = -10000\n",
    "\n",
    "    for t1 in data.index:\n",
    "        for t2 in data.index:\n",
    "            if t1<=t2:\n",
    "                drawdown = data[t1] - data[t2]\n",
    "                if drawdown > max_dd:\n",
    "                    max_dd = drawdown\n",
    "    \n",
    "    return max_dd\n",
    "\n",
    "# Helper function to calculate \n",
    "\n",
    "def turnover(returns, weights):\n",
    "    turnovers = list()\n",
    "    for date1, date2 in zip(returns.index.unique(), returns.index.unique()[1:]):\n",
    "        w_t_1 = weights.loc[date1]\n",
    "        w_t_2 = weights.loc[date2]\n",
    "        r_t = returns.loc[date1]\n",
    "\n",
    "        permno = set(w_t_1['permno']).intersection(set(w_t_2['permno']))\n",
    "        w_t_1 = w_t_1[w_t_1['permno'].isin(permno)]['weight']\n",
    "        w_t_2 = w_t_2[w_t_2['permno'].isin(permno)]['weight']\n",
    "        r_t = r_t[r_t['permno'].isin(permno)]['actual']\n",
    "\n",
    "        wr = w_t_1*(1+r_t)\n",
    "        wr_sum = 1+(r_t*w_t_1).sum()\n",
    "\n",
    "        turnover = np.abs(w_t_2.values - (wr/wr_sum).values).sum()\n",
    "        turnovers.append(turnover)\n",
    "        \n",
    "    return turnovers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Portfolio excess return\n",
    "portfolio_summary['excess_return'] = portfolio_summary['return'].copy()\n",
    "portfolio_summary['excess_return'] = pd.merge(portfolio_summary['excess_return'], risk_free, on='date').set_index('date')\n",
    "\n",
    "for col in portfolio_summary['excess_return'].columns:\n",
    "    portfolio_summary['excess_return'][col] = portfolio_summary['excess_return'][col] - portfolio_summary['excess_return']['Rfree'] # Calculate excess returns\n",
    "portfolio_summary['excess_return'].drop('Rfree', axis=1, inplace=True)\n",
    "portfolio_summary['return'] = portfolio_summary['return'].set_index('date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate portfolio performance evaluation metrics\n",
    "\n",
    "portfolio_analysis = pd.DataFrame()\n",
    "\n",
    "# (1) Monthly mean return\n",
    "portfolio_analysis['mean return'] = portfolio_summary['return'].mean()\n",
    "\n",
    "# (2) Cumulative log return\n",
    "cumsum = portfolio_summary['return'].cumsum()\n",
    "portfolio_analysis['cumulative return'] = cumsum.iloc[-1]\n",
    "\n",
    "# (3) Annualizedd Sharpe ratio\n",
    "annualized_return  = portfolio_summary['excess_return'].mean()\n",
    "annualized_volatility  = portfolio_summary['return'].std()\n",
    "portfolio_analysis['Sharpe'] = annualized_return *  np.sqrt(12) /annualized_volatility\n",
    "\n",
    "# (4) Maximum 1 month loss\n",
    "portfolio_analysis['max 1M loss'] = portfolio_summary['return'].min().abs() * 100\n",
    "\n",
    "# (5) Maximum Drawdown\n",
    "portfolio_analysis['max DD'] = cumsum.apply(MAX_DD) * 100\n",
    "\n",
    "# (6) Portfolio Turnover\n",
    "portfolio_turnover = pd.Series()\n",
    "\n",
    "for weight in portfolio_summary['weight']:\n",
    "    returns = portfolio_summary['returns'][weight]\n",
    "    weights = portfolio_summary['weight'][weight]\n",
    "\n",
    "    portfolio_turnover[weight] = np.mean(turnover(returns, weights))\n",
    "\n",
    "portfolio_analysis['turnover'] = portfolio_turnover * 100\n",
    "portfolio_analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyze predicted consensus variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = dict()\n",
    "consensus_names = ['AnalystRevision',\n",
    "'ChangeInRecommendation',\n",
    "'ChForecastAccrual' ,\n",
    "'EarningsForecastDisparity' ,\n",
    "'FEPS' ,\n",
    "'ForecastDispersion' ,\n",
    "'REV6' ,\n",
    "'AnalystValue',\n",
    "'AOP']\n",
    "\n",
    "\n",
    "for folder in os.listdir('checkpoints'):\n",
    "    if folder.startswith(HRIZON):\n",
    "        weight_lambda = float(folder.split(HRIZON+'_')[1])\n",
    "        path = os.path.join('checkpoints', folder)\n",
    "        results[weight_lambda] = pd.DataFrame()\n",
    "\n",
    "        for file in os.listdir(path):\n",
    "            file_path = os.path.join(path, file)\n",
    "            weights = torch.load(file_path, map_location=torch.device('cpu')) # Load trained model parameters\n",
    "            date = file.split('model_')[0]\n",
    "            \n",
    "            # Load model weights and biases for return module (Linear regression coefficients of consensus varialbes and asset returns)\n",
    "            if date in results[weight_lambda].columns:\n",
    "                results[weight_lambda][date] =  results[weight_lambda][date].add(pd.Series(torch.cat((weights['final_model.weight'][0], weights['final_model.bias']))))\n",
    "            else:\n",
    "                results[weight_lambda][date] = torch.cat((weights['final_model.weight'][0], weights['final_model.bias']))\n",
    "\n",
    "        # Get mean coefficients for each ensemble models\n",
    "        results[weight_lambda].index = consensus_names + ['bias']    \n",
    "        results[weight_lambda] = results[weight_lambda]/10\n",
    "        results[weight_lambda] = results[weight_lambda].mean(axis=1)\n",
    "\n",
    "results = pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "fig, axs = plt.subplots(nrows=5, ncols=2, sharex=True, sharey=True, figsize=(10, 15))\n",
    "\n",
    "for col in results.columns:\n",
    "    if col != 0.0:\n",
    "        index = np.arange(len(columns))\n",
    "        ax = fig.axes[i]        \n",
    "        # Color red and blue for positive and negative coefficients respectively\n",
    "        ax.bar(index-0.4, results[col], width=0.4, align='center', color=np.where(results[col]>0, 'red', 'blue'))\n",
    "        ax.set_xticks(index-0.3)\n",
    "        ax.set_xticklabels(results[col].index, rotation=90)\n",
    "        ax.grid(axis=\"y\")\n",
    "        ax.set_title(f\"$\\\\lambda =$ {str(col)}\")\n",
    "        i = i+1\n",
    "    #results[col].plot.bar(color=np.where(results[col]>0, 'red', 'blue'))\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.mean(axis=1).plot.barh(color=np.where(results[col]>0, 'red', 'blue'))\n",
    "plt.grid(axis=\"x\")\n",
    "plt.gca().invert_yaxis()\n",
    "plt.show()\n",
    "plt.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cb_apm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
